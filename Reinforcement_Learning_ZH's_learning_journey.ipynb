{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nov 5th, 2024, work in progress.\n",
        "What is RL?\n",
        "- RL is the learning process guided by trial-n-errors, in which policy will guide actions, values will guide policies\n",
        "\n",
        "\n",
        "https://arena3-chapter2-rl.streamlit.app/%5B2.2%5D_Q-Learning_and_DQN\n",
        "\n",
        "What needs to be learned?\n",
        "\n",
        "Where is the gap?"
      ],
      "metadata": {
        "id": "59mzg1ZJjiwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Daily Records\n",
        "- Nov 5th: Understanding value function conceptually and in math"
      ],
      "metadata": {
        "id": "ETb_5eST2Zip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conceptual Understanding\n",
        "value function\n",
        "- Has been a pain to understand what is really a value function and what it does to the agent exploration. I think the best explanation I got so far is valuction function is a scoreboard that states accumulative values associated to each path. It does not pick up policy as I initially imagined, but it conisders success rate, panalty along the pathes and reflect all these in the final scores.\n",
        "- But still, value function does not choose certain path or action definitely because you have to consider intrinsic uncertainties etc. It's more like \"value function favors pathA\" but policy function can associate uncertainty to how much they believe the judgement of value funciton, and how much they would think the judgement evoles (dynamic!)."
      ],
      "metadata": {
        "id": "EwihZhW6uI2q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bIgCsyxizXf"
      },
      "outputs": [],
      "source": [
        "# Pseudocode of a simple RL algorithm\n",
        "# Board/World  # A board to reflect how the current world state evolves. In a simple tic tac toe game, the environ is just given; whereas in realistic case it should be a model of the envirionment.\n",
        "\n",
        "\n",
        "# Node/Agent  # Actions agent can take\n",
        "\n",
        "# RL training algorithm: to obtain a policy model according to wjich agents can take actions.\n",
        "def RL_training():\n",
        "\n",
        "  return policy\n",
        "# RL action Algorithm\n",
        "def rl_action():\n",
        "  policy = value(state) # my understanding is this is user-defined\n",
        "  action = policy(state)\n",
        "  return action\n"
      ]
    }
  ]
}