{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nov 4th, 2024, work in progress.\n",
        "What is RL?\n",
        "- RL is the learning process guided by trial-n-errors, in which policy will guide actions, values will guide policies\n",
        "\n",
        "\n",
        "https://arena3-chapter2-rl.streamlit.app/%5B2.2%5D_Q-Learning_and_DQN\n",
        "\n",
        "What needs to be learned?\n",
        "\n",
        "Where is the gap?"
      ],
      "metadata": {
        "id": "59mzg1ZJjiwY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bIgCsyxizXf"
      },
      "outputs": [],
      "source": [
        "# Pseudocode of a simple RL algorithm\n",
        "# Board/World  # A board to reflect how the current world state evolves. In a simple tic tac toe game, the environ is just given; whereas in realistic case it should be a model of the envirionment.\n",
        "\n",
        "\n",
        "# Node/Agent  # Actions agent can take\n",
        "\n",
        "# RL training algorithm: to obtain a policy model according to wjich agents can take actions.\n",
        "def RL_training():\n",
        "\n",
        "  return policy\n",
        "# RL action Algorithm\n",
        "def rl_action():\n",
        "  policy = value(state) # my understanding is this is user-defined\n",
        "  action = policy(state)\n",
        "  return action\n"
      ]
    }
  ]
}